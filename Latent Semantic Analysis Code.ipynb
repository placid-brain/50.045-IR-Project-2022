{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/placid_brain/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/placid_brain/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import copy\n",
    "sw = set(stopwords.words('english'))\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"/Users/placid_brain/Documents/IR stuff/lsa/recipes_w_search_terms.csv\",nrows=1000)\n",
    "\n",
    "def function(ini_list):\n",
    "    new_cell = ini_list.strip('][').split(', ')\n",
    "    for item in new_cell:\n",
    "        item = item.replace(\"'\",\"\")\n",
    "    return new_cell\n",
    "\n",
    "df1['ingredients'] = df1['ingredients'].apply(function)\n",
    "\n",
    "\n",
    "df2 = df1[['id', 'name','ingredients','steps']]\n",
    "#print(df2.head())\n",
    "\n",
    "ingredients = df2['ingredients'].tolist()\n",
    "\n",
    "new_lst =  [' '.join(i) for i in ingredients]\n",
    "\n",
    "tfvec = TfidfVectorizer()\n",
    "# turning ingredients-dishes(doc id) into tf-df vector\n",
    "tdf = tfvec.fit_transform(new_lst).T\n",
    "# singular value decomposition\n",
    "[dc,cs,tc] = np.linalg.svd(tdf.toarray(), full_matrices=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSA:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def query_vector_gen(self,q_list):\n",
    "        # turning query into tf-idf vector\n",
    "        q = tfvec.transform(q_list)\n",
    "        q = q.toarray()\n",
    "        # mapping query vector to document-concept matrix and eventually concept strength matrix\n",
    "        q_dc = np.matmul(q,dc)\n",
    "        q_dc__s = np.matmul(q_dc,np.linalg.inv(np.diag(cs)))\n",
    "        return q_dc__s\n",
    "    \n",
    "    def search(self, query):\n",
    "        query = query.lower()\n",
    "        \n",
    "        query_tokens = word_tokenize(query)\n",
    "        q_list = []\n",
    "\n",
    "        for w in query_tokens:\n",
    "            if w not in sw:\n",
    "                q_list.append(w)\n",
    "        res = \" \".join(q_list)\n",
    "        final_list =[]\n",
    "        final_list.append(res)\n",
    "        \n",
    "\n",
    "        result = self.query_vector_gen(final_list)\n",
    "        \n",
    "        lst = []\n",
    "\n",
    "        for arr in range(len(tc)):\n",
    "            # mapping resultant vector to term-concept matrix\n",
    "            lst.append(np.dot(result,tc[:,arr]))\n",
    "        \n",
    "        temp_lst = copy.deepcopy(lst)\n",
    "        desc_lst = sorted(lst,reverse=True)[0:5]\n",
    "        for i in range(len(desc_lst)):\n",
    "            \n",
    "            print(\"{}. {}\".format(i+1, df2[\"name\"][temp_lst.index(desc_lst[i])]))\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Lemon Dressing\n",
      "2. Christine's Super Chocolate Cake\n",
      "3. Blueberry Waffles\n",
      "4. Almond Kulich\n",
      "5. Mom Hart's Pie Crust\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lsa = LSA()\n",
    "start = time.time()\n",
    "print(lsa.search('sAlt crEAm and rice and buTter water ice lemon pepper'))\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken:  0.08963418006896973\n"
     ]
    }
   ],
   "source": [
    "print(\"Time taken: \", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
